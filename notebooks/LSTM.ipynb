{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-24T16:18:31.130822Z",
     "start_time": "2024-06-24T16:18:31.126067Z"
    }
   },
   "source": [
    "from src.config import DEFAULT_DATA_FOLDER\n",
    "from src.models.lstm.utils import determine_labels\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from typing import Union, Tuple, List\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T16:18:31.337001Z",
     "start_time": "2024-06-24T16:18:31.132613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATASET_PATH = f\"{DEFAULT_DATA_FOLDER}/output/combined_posts_results.xlsx\"\n",
    "dataset = pd.read_excel(DATASET_PATH)"
   ],
   "id": "c5e9fa16fc45101",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T16:18:38.545388Z",
     "start_time": "2024-06-24T16:18:31.337815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load pre-trained Word2Vec embeddings (example)\n",
    "word2vec_path = f'{DEFAULT_DATA_FOLDER}/word-embedding/SO_vectors_200.bin'\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ],
   "id": "9343ab8f1169209b",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T16:18:38.563023Z",
     "start_time": "2024-06-24T16:18:38.547222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# labels the columns\n",
    "dataset['category'] = dataset.apply(determine_labels, axis=1)\n",
    "dataset['clean_text'] = dataset['clean_text'].fillna('').astype(str)\n",
    "\n",
    "label_encoder: LabelEncoder = LabelEncoder()\n",
    "dataset['category'] = label_encoder.fit_transform(dataset['category'])\n",
    "num_classes: int = len(label_encoder.classes_)"
   ],
   "id": "579d6142e9370e70",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T16:18:38.567241Z",
     "start_time": "2024-06-24T16:18:38.563867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset['clean_text'], dataset['category'], test_size=0.2,\n",
    "                                                    random_state=42)"
   ],
   "id": "f5b607723cbeabd3",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-06-24T16:18:38.568380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokenize and pad sequences\n",
    "max_num_words: int = dataset['clean_text'].str.len().max()\n",
    "print(\"Max number of words: \", max_num_words)\n",
    "max_sequence_length: int = 250\n",
    "tokenizer: Tokenizer = Tokenizer(num_words=max_num_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)"
   ],
   "id": "56666d9f69a59250",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "X_train_padded: np.ndarray = pad_sequences(X_train_sequences, maxlen=max_sequence_length)\n",
    "X_test_padded: np.ndarray = pad_sequences(X_test_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Convert labels to categorical\n",
    "y_train_categorical: np.ndarray = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_categorical: np.ndarray = to_categorical(y_test, num_classes=num_classes)"
   ],
   "id": "280e56de91d06016",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "sequences = tokenizer.texts_to_sequences(dataset['clean_text'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "labels_categorical: np.ndarray = to_categorical(dataset['category'], num_classes=num_classes)"
   ],
   "id": "8404e31dacfa82b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "fold_no = 1\n",
    "accuracies: List[float] = []\n",
    "\n",
    "for train_index, test_index in skf.split(padded_sequences, dataset['category']):\n",
    "    print(f'Training on fold {fold_no}...')\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test = padded_sequences[train_index], padded_sequences[test_index]\n",
    "    y_train, y_test = labels_categorical[train_index], labels_categorical[test_index]\n",
    "\n",
    "    # Build the LSTM model\n",
    "    # embedding_dim: int = 100\n",
    "    embedding_dim = word2vec_model.vector_size\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))  # word_index: your vocabulary index\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if word in word2vec_model:\n",
    "            embedding_matrix[i] = word2vec_model[word]\n",
    "\n",
    "    model: Sequential = Sequential()\n",
    "    \n",
    "    # Build the LSTM model with pre-trained embeddings\n",
    "    model.add(Embedding(input_dim=len(word_index) + 1, output_dim=embedding_dim,\n",
    "                        embeddings_initializer=Constant(embedding_matrix),\n",
    "                        trainable=False))  # Set trainable=False to use pre-trained embeddings\n",
    "    \n",
    "    # model.add(Embedding(input_dim=max_num_words, output_dim=embedding_dim))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "    # Define early stopping callback\n",
    "    # early_stopping = EarlyStopping(monitor='val_loss', patience=3, mode='min', restore_best_weights=True)\n",
    "    # \n",
    "    # # Train the model with early stopping\n",
    "    # history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1,\n",
    "    #                     callbacks=[early_stopping], verbose=2)\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=40, batch_size=32, validation_split=0.1,\n",
    "                     verbose=2)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "    print(f'Test Accuracy for fold {fold_no}: {accuracy}')\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    fold_no += 1"
   ],
   "id": "2af2aade408589cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate the average accuracy across all folds\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f'Average Test Accuracy: {average_accuracy}')"
   ],
   "id": "182b3a70a5135df5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "model.summary()",
   "id": "43cab00a8ebb505d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "d92c5e64a8ec29ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "827e16d60d82d149",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
