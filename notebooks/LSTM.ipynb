{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-01T12:28:14.178314Z",
     "start_time": "2024-07-01T12:28:09.126467Z"
    }
   },
   "source": [
    "from src.config import DEFAULT_DATA_FOLDER\n",
    "from src.models.lstm.utils import determine_labels\n",
    "from src.data.utils import save_as_excel\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from gensim.models import KeyedVectors\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T12:28:21.134699Z",
     "start_time": "2024-07-01T12:28:14.179610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load datasets\n",
    "# Load the dataset\n",
    "DATASET_PATH = f\"{DEFAULT_DATA_FOLDER}/output/combined_posts_results.xlsx\"\n",
    "dataset = pd.read_excel(DATASET_PATH)\n",
    "\n",
    "# Load pre-trained Word2Vec embeddings\n",
    "word2vec_path = f'{DEFAULT_DATA_FOLDER}/word-embedding/SO_vectors_200.bin'\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(word2vec_path,binary=True)"
   ],
   "id": "2171cf672e74037",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T12:28:21.153827Z",
     "start_time": "2024-07-01T12:28:21.135591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare the dataset\n",
    "dataset['category'] = dataset.apply(determine_labels, axis=1)\n",
    "dataset['clean_text'] = dataset['clean_text'].fillna('').astype(str)"
   ],
   "id": "2b66636ae2edb081",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T12:28:21.158077Z",
     "start_time": "2024-07-01T12:28:21.155332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "label_encoder = LabelEncoder()\n",
    "dataset['category'] = label_encoder.fit_transform(dataset['category'])\n",
    "num_classes = len(label_encoder.classes_)"
   ],
   "id": "3d4cfe660e0066f2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T12:28:21.453941Z",
     "start_time": "2024-07-01T12:28:21.158928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokenize and pad sequences\n",
    "max_num_words = dataset['clean_text'].str.len().max()\n",
    "max_sequence_length = 250\n",
    "tokenizer = Tokenizer(num_words=max_num_words)\n",
    "tokenizer.fit_on_texts(dataset['clean_text'])\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(dataset['clean_text'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "labels_categorical = to_categorical(dataset['category'], num_classes=num_classes)"
   ],
   "id": "4506052082ae57e1",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T12:28:21.456874Z",
     "start_time": "2024-07-01T12:28:21.454764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# hyper-parameters: these are calculated using optimize parameters notebook\n",
    "embedding_dim = 200\n",
    "lstm_units = 100\n",
    "dropout_rate = 0.2\n",
    "batch_size = 16\n",
    "epochs = 20"
   ],
   "id": "879a9fd3e3ff5422",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T12:28:21.459725Z",
     "start_time": "2024-07-01T12:28:21.457656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "fold_no = 1"
   ],
   "id": "62457c5a0ae9768d",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-07-01T12:28:21.460632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fold_accuracies = []\n",
    "fold_precisions = []\n",
    "fold_recalls = []\n",
    "fold_f1_scores = []\n",
    "\n",
    "# To store the results\n",
    "results = []\n",
    "\n",
    "for train_index, test_index in kf.split(padded_sequences, dataset['category']):\n",
    "    print(f'Training on fold {fold_no} with params: embedding_dim={embedding_dim}, lstm_units={lstm_units}, dropout_rate={dropout_rate}, batch_size={batch_size}')\n",
    "\n",
    "    X_train, X_test = padded_sequences[train_index], padded_sequences[test_index]\n",
    "    y_train, y_test = labels_categorical[train_index], labels_categorical[test_index]\n",
    "\n",
    "    # Build the LSTM model with pre-trained embeddings\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if word in word2vec_model:\n",
    "            embedding_matrix[i] = word2vec_model[word]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(word_index) + 1, output_dim=embedding_dim,\n",
    "                        embeddings_initializer=Constant(embedding_matrix),\n",
    "                        trainable=False))\n",
    "    model.add(SpatialDropout1D(dropout_rate))\n",
    "    model.add(Bidirectional(LSTM(lstm_units, dropout=dropout_rate, recurrent_dropout=dropout_rate)))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, mode='min', restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1,\n",
    "                        callbacks=[early_stopping], verbose=2)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test, verbose=2)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "    accuracy = np.mean(y_pred_classes == y_true)\n",
    "    precision = precision_score(y_true, y_pred_classes, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred_classes, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred_classes, average='weighted')\n",
    "\n",
    "    print(f'Test Accuracy for fold {fold_no}: {accuracy}')\n",
    "    print(f'Test Precision for fold {fold_no}: {precision}')\n",
    "    print(f'Test Recall for fold {fold_no}: {recall}')\n",
    "    print(f'Test F1 Score for fold {fold_no}: {f1}')\n",
    "\n",
    "    fold_accuracies.append(accuracy)\n",
    "    fold_precisions.append(precision)\n",
    "    fold_recalls.append(recall)\n",
    "    fold_f1_scores.append(f1)\n",
    "\n",
    "    # Store the results\n",
    "    fold_results = pd.DataFrame({\n",
    "        'fold': fold_no,\n",
    "        'true': y_true,\n",
    "        'predicted': y_pred_classes\n",
    "    })\n",
    "    results.append(fold_results)\n",
    "\n",
    "    fold_no += 1\n",
    "    \n",
    "    save_as_excel(fold_results, f\"{DEFAULT_DATA_FOLDER}/validation_results/fold{fold_no}.xlsx\")\n",
    "    \n",
    "print(\"Fold accuracies: \", fold_accuracies)\n",
    "print(\"Fold precisions: \", fold_precisions)\n",
    "print(\"Fold recalls: \", fold_recalls)\n",
    "print(\"Fold f1 scores: \", fold_f1_scores)"
   ],
   "id": "369167f79b49f42",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1 with params: embedding_dim=200, lstm_units=100, dropout_rate=0.2, batch_size=16\n",
      "Epoch 1/20\n",
      "121/121 - 21s - 171ms/step - accuracy: 0.6341 - loss: 0.9632 - val_accuracy: 0.9628 - val_loss: 0.2101\n",
      "Epoch 2/20\n",
      "121/121 - 24s - 200ms/step - accuracy: 0.7084 - loss: 0.7500 - val_accuracy: 0.9116 - val_loss: 0.2747\n",
      "Epoch 3/20\n",
      "121/121 - 29s - 241ms/step - accuracy: 0.7447 - loss: 0.6532 - val_accuracy: 0.9070 - val_loss: 0.2502\n",
      "Epoch 4/20\n",
      "121/121 - 29s - 237ms/step - accuracy: 0.7815 - loss: 0.5609 - val_accuracy: 0.9070 - val_loss: 0.1970\n",
      "Epoch 5/20\n",
      "121/121 - 23s - 187ms/step - accuracy: 0.8199 - loss: 0.4873 - val_accuracy: 0.9395 - val_loss: 0.1455\n",
      "Epoch 6/20\n",
      "121/121 - 26s - 215ms/step - accuracy: 0.8350 - loss: 0.4594 - val_accuracy: 0.8930 - val_loss: 0.2506\n",
      "Epoch 7/20\n",
      "121/121 - 27s - 226ms/step - accuracy: 0.8625 - loss: 0.3929 - val_accuracy: 0.9163 - val_loss: 0.2161\n",
      "Epoch 8/20\n",
      "121/121 - 27s - 222ms/step - accuracy: 0.8739 - loss: 0.3554 - val_accuracy: 0.9535 - val_loss: 0.1299\n",
      "Epoch 9/20\n",
      "121/121 - 28s - 235ms/step - accuracy: 0.8879 - loss: 0.3125 - val_accuracy: 0.9349 - val_loss: 0.1391\n",
      "Epoch 10/20\n",
      "121/121 - 28s - 231ms/step - accuracy: 0.9081 - loss: 0.2661 - val_accuracy: 0.9581 - val_loss: 0.1162\n",
      "Epoch 11/20\n",
      "121/121 - 28s - 235ms/step - accuracy: 0.9242 - loss: 0.2445 - val_accuracy: 0.8930 - val_loss: 0.2470\n",
      "Epoch 12/20\n",
      "121/121 - 26s - 216ms/step - accuracy: 0.9362 - loss: 0.2106 - val_accuracy: 0.9163 - val_loss: 0.1953\n",
      "Epoch 13/20\n",
      "121/121 - 29s - 237ms/step - accuracy: 0.9331 - loss: 0.1983 - val_accuracy: 0.9674 - val_loss: 0.0814\n",
      "Epoch 14/20\n",
      "121/121 - 27s - 219ms/step - accuracy: 0.9414 - loss: 0.1733 - val_accuracy: 0.9581 - val_loss: 0.1131\n",
      "Epoch 15/20\n",
      "121/121 - 26s - 217ms/step - accuracy: 0.9559 - loss: 0.1559 - val_accuracy: 0.9395 - val_loss: 0.1595\n",
      "Epoch 16/20\n",
      "121/121 - 28s - 230ms/step - accuracy: 0.9600 - loss: 0.1375 - val_accuracy: 0.9395 - val_loss: 0.1993\n",
      "8/8 - 1s - 105ms/step\n",
      "Test Accuracy for fold 1: 0.7352941176470589\n",
      "Test Precision for fold 1: 0.7111998879248094\n",
      "Test Recall for fold 1: 0.7352941176470589\n",
      "Test F1 Score for fold 1: 0.7224118417479388\n",
      "Training on fold 2 with params: embedding_dim=200, lstm_units=100, dropout_rate=0.2, batch_size=16\n",
      "Epoch 1/20\n",
      "121/121 - 17s - 141ms/step - accuracy: 0.6191 - loss: 0.9553 - val_accuracy: 0.9395 - val_loss: 0.1951\n",
      "Epoch 2/20\n",
      "121/121 - 18s - 151ms/step - accuracy: 0.7302 - loss: 0.7016 - val_accuracy: 0.9442 - val_loss: 0.1900\n",
      "Epoch 3/20\n",
      "121/121 - 20s - 163ms/step - accuracy: 0.7556 - loss: 0.6178 - val_accuracy: 0.9535 - val_loss: 0.1518\n",
      "Epoch 4/20\n",
      "121/121 - 22s - 182ms/step - accuracy: 0.7872 - loss: 0.5612 - val_accuracy: 0.9209 - val_loss: 0.2522\n",
      "Epoch 5/20\n",
      "121/121 - 22s - 180ms/step - accuracy: 0.8116 - loss: 0.5003 - val_accuracy: 0.9535 - val_loss: 0.1257\n",
      "Epoch 6/20\n",
      "121/121 - 22s - 180ms/step - accuracy: 0.8376 - loss: 0.4503 - val_accuracy: 0.9581 - val_loss: 0.1627\n",
      "Epoch 7/20\n",
      "121/121 - 23s - 190ms/step - accuracy: 0.8583 - loss: 0.3977 - val_accuracy: 0.9256 - val_loss: 0.2404\n",
      "Epoch 8/20\n",
      "121/121 - 21s - 172ms/step - accuracy: 0.8692 - loss: 0.3707 - val_accuracy: 0.9395 - val_loss: 0.1731\n",
      "8/8 - 1s - 96ms/step\n",
      "Test Accuracy for fold 2: 0.7605042016806722\n",
      "Test Precision for fold 2: 0.7294455572926846\n",
      "Test Recall for fold 2: 0.7605042016806722\n",
      "Test F1 Score for fold 2: 0.7377460980700161\n",
      "Training on fold 3 with params: embedding_dim=200, lstm_units=100, dropout_rate=0.2, batch_size=16\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adarshchoudhary/Documents/University of Paderborn/Data Science of Software Engineering/Assignments/Assignment 3/code/.env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121/121 - 22s - 186ms/step - accuracy: 0.6367 - loss: 0.9526 - val_accuracy: 0.9767 - val_loss: 0.1850\n",
      "Epoch 2/20\n",
      "121/121 - 22s - 183ms/step - accuracy: 0.7182 - loss: 0.7213 - val_accuracy: 0.9814 - val_loss: 0.1738\n",
      "Epoch 3/20\n",
      "121/121 - 23s - 193ms/step - accuracy: 0.7483 - loss: 0.6476 - val_accuracy: 0.9628 - val_loss: 0.1502\n",
      "Epoch 4/20\n",
      "121/121 - 22s - 178ms/step - accuracy: 0.7800 - loss: 0.5676 - val_accuracy: 0.9674 - val_loss: 0.1417\n",
      "Epoch 5/20\n",
      "121/121 - 20s - 164ms/step - accuracy: 0.8044 - loss: 0.5108 - val_accuracy: 0.8558 - val_loss: 0.3644\n",
      "Epoch 6/20\n",
      "121/121 - 20s - 166ms/step - accuracy: 0.8251 - loss: 0.4600 - val_accuracy: 0.9535 - val_loss: 0.1334\n",
      "Epoch 7/20\n",
      "121/121 - 21s - 170ms/step - accuracy: 0.8635 - loss: 0.3896 - val_accuracy: 0.9302 - val_loss: 0.2026\n",
      "Epoch 8/20\n",
      "121/121 - 21s - 176ms/step - accuracy: 0.8718 - loss: 0.3619 - val_accuracy: 0.9535 - val_loss: 0.1333\n",
      "Epoch 9/20\n",
      "121/121 - 23s - 192ms/step - accuracy: 0.8936 - loss: 0.3139 - val_accuracy: 0.9209 - val_loss: 0.1960\n",
      "Epoch 10/20\n",
      "121/121 - 25s - 208ms/step - accuracy: 0.8983 - loss: 0.2850 - val_accuracy: 0.9209 - val_loss: 0.2016\n",
      "Epoch 11/20\n",
      "121/121 - 24s - 195ms/step - accuracy: 0.9165 - loss: 0.2511 - val_accuracy: 0.9163 - val_loss: 0.2436\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x3220c4d60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "8/8 - 1s - 92ms/step\n",
      "Test Accuracy for fold 3: 0.7647058823529411\n",
      "Test Precision for fold 3: 0.7682123873414948\n",
      "Test Recall for fold 3: 0.7647058823529411\n",
      "Test F1 Score for fold 3: 0.7604837056773929\n",
      "Training on fold 4 with params: embedding_dim=200, lstm_units=100, dropout_rate=0.2, batch_size=16\n",
      "Epoch 1/20\n",
      "121/121 - 28s - 229ms/step - accuracy: 0.6279 - loss: 0.9826 - val_accuracy: 0.9674 - val_loss: 0.2357\n",
      "Epoch 2/20\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate average metrics for current hyperparameters\n",
    "average_accuracy = np.mean(fold_accuracies)\n",
    "average_precision = np.mean(fold_precisions)\n",
    "average_recall = np.mean(fold_recalls)\n",
    "average_f1_score = np.mean(fold_f1_scores)\n",
    "\n",
    "print(f'Average Test Accuracy for config {embedding_dim}, {lstm_units}, {dropout_rate}, {batch_size}: {average_accuracy}')\n",
    "print(f'Average Test Precision for config {embedding_dim}, {lstm_units}, {dropout_rate}, {batch_size}: {average_precision}')\n",
    "print(f'Average Test Recall for config {embedding_dim}, {lstm_units}, {dropout_rate}, {batch_size}: {average_recall}')\n",
    "print(f'Average Test F1 Score for config {embedding_dim}, {lstm_units}, {dropout_rate}, {batch_size}: {average_f1_score}')"
   ],
   "id": "a36305d06317eb29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Plot the training history of the best model (for demonstration, assuming the last trained model is the best)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(f\"{DEFAULT_DATA_FOLDER}/charts/Training_loss_vs_loss.png\")"
   ],
   "id": "570104ec689598fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "# Ensure the model directory exists\n",
    "model_dir = os.path.join(DEFAULT_DATA_FOLDER, \"models\")\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "    \n",
    "model.save(f\"{DEFAULT_DATA_FOLDER}/models/lstm.keras\")"
   ],
   "id": "e0849e47d4dcc5d0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
